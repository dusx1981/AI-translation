# Code Llama 概述

## Code Llama 介绍
- 大型语言模型已经在自然语言处理领域取得了显著的进步，这些模型可以执行多种任务，包括但不限于文本生成、对话系统、问答系统等。
- 它们通过在大量文本数据上进行训练来学习语言结构和模式。随着技术的进步，这些模型现在也被用于代码相关的任务，例如编写代码、代码补全、代码解释等。

## Code Llama 的背景
- Code Llama 是一组基于 Llama 2 的大型语言模型，专注于代码生成和代码补全任务。
- Llama 2 本身是一个通用的大型语言模型，它在广泛的文本数据上进行了预训练，包括一般的自然语言文本和代码数据。

## 训练方法
- AlphaCode、InCoder 和 StarCoder 等模型都是专门为代码生成而设计的，它们通常只在代码数据集上进行训练。
- Codex 则是从一个通用的语言模型出发，然后在这个基础上进行微调，使其适应代码相关的任务。
- Code Llama 同样采取类似的方法，即从一个已经预训练过的通用模型      开始，然后通过在代码数据上进行额外的训练和微调，使模型能够更好地理解和生成代码。

## 训练策略
- 初始化：Code Llama 使用 Llama 2 作为起点，这意味着它在开始专注于代码之前就已经具备了一定的自然语言处理能力。
- 训练流程：模型经历了多阶段的训练过程，包括基础训练、代码训练以及可能的进一步微调，以提高其在特定代码任务上的性能。
- 性能对比：通过实验比较发现，使用 Llama 2 进行初始化的 Code Llama 模型，在同样的训练资源条件下，相较于直接从零开始仅在代码数据上训练的模型，表现更好。

## 训练细节
- 这些模型在含有 16,000 个令牌的序列上进行了训练，并且在处理长达 100,000 个令牌的输入时显示出更好的性能。
- 特别是，70 亿、130 亿和 700 亿参数的 Code Llama 和 Code Llama - Instruct 变体支持基于周围内容的代码填充功能。

## 性能指标
- HumanEval：评估模型能否根据自然语言描述正确生成函数实现。
- MBPP (Microsoft's Benchmark for Programming Problems)：衡量模型解决编程问题的能力。
- MultiPL-E：一个多语言编程基准测试。

## 填充（Infilling）
- 定义：填充是指在已知文本序列中填入缺失的部分，同时考虑到上下文的一致性和逻辑性。
- 问题：传统的自回归训练（即基于先前的词汇预测下一个词汇的方法）和微调过程使模型能够完成给定的提示，但通常缺乏在考虑整体上下文的情况下填补缺失文本片段的能力。
- 解决方案：通过多任务学习方法，结合<kbd>自回归预测</kbd>（即基于已有词汇预测后续词汇）和 <kbd>因果填充预测</kbd>（即在已知 **周围词汇** 的情况下预测缺失词汇），来增强模型的填充能力。
- 应用场景：这种技术特别适用于源代码编辑器中的实时代码补全功能或文档字符串的生成。

## 长输入上下文
- 挑战：为了处理更大范围的代码（例如整个项目或仓库），模型需要理解比单一函数或文件更长的上下文。
- 限制：原始的Llama 2模型支持的最大上下文长度为4,096个令牌（token）。
- 改进：通过调整旋转位置嵌入（RoPE）参数，增加了一个额外的微调阶段，从而扩展了模型的最大上下文长度到100,000个令牌。
- 效果：这使得模型能够有效地处理更长的上下文，对代码生成的质量影响较小。

## 指令微调
- 定义：指令微调是一种训练技术，通过这种方式，模型被指导执行特定任务，以改进其响应的准确性和适用性。
- 目的：提高模型的安全性、减少有害输出以及避免偏见。
- 方法：使用专有的指令数据集进行额外的微调，这些数据集包含特定的指令和预期的响应。
- 数据集：***使用了一种新的自我指令数据集，它是由向Llama 2提问编码问题，然后让Code Llama生成相关的单元测试和解决方案的方式构建的。***
- 效果：改进后的模型（Code Llama - Instruct）在真实世界测试、有害性检测和偏见基准上表现出更好的性能，同时对代码生成的质量影响较小。

## 模型变体
- 基础模型 (Code Llama)：通用的基础模型，适用于广泛的编程任务。
- Python 专项模型 (Code Llama - Python)：专门针对 Python 编程语言进行了优化，以提高在 Python 相关任务中的表现。
- 遵循指令的模型 (Code Llama - Instruct)：经过训练能够更好地理解并遵循用户给出的指令或提示。
- 每种类型的模型都有四个不同大小的版本，参数量分别为 70 亿、130 亿、340 亿和 700 亿个。
- 示意图：
![模型](/imgs/models.png)

## 应用场景
- 程序合成：根据自然语言描述生成相应的程序代码。
- 代码补全：自动完成部分已写好的代码。
- 调试辅助：帮助开发者找出并修复代码中的错误。
- 文档生成：自动生成代码注释或文档。


# Code Llama：专门为代码设计

## 代码Llama模型家族

1. 代码Llama 模型
    模型规模：该家族包含了四个不同规模的模型，分别是7B、13B、34B和70B参数。这些数字代表了模型中可训练参数的数量。
    - 训练方式：
        - 7B、13B和70B的模型使用了一种叫做 ***填充（infilling）*** 的目标进行训练。这种训练方法有助于模型在代码片段中间补全缺失的部分，使其非常适合在IDE中完成代码片段。
        - 34B模型则没有使用填充目标进行训练。
    - 初始化与数据集：
        - 所有的代码Llama模型都从Llama 2模型的权重开始训练。
        - 除70B模型外的所有模型都在包含大量代码的500亿标记数据集上进行训练。
        - 70B模型则在更大的数据集上训练，总共1万亿标记。
        - 长上下文处理：所有模型都经过了特殊的微调过程来处理长上下文，即能够理解并生成更长的代码段。

2. 代码Llama - Python 模型
    - 专业领域：这些模型专门针对Python代码生成进行了优化。
    - 模型规模：同样提供了7B、13B、34B和70B参数的不同规模。
    - 训练方式：
        - 这些模型从Llama 2模型初始化，并在500亿标记的代码Llama数据集上进行训练。
        - 在此基础上，还使用了一个包含100亿标记的Python代码数据集进行了额外的专业化训练。
        - 7B、13B和34B参数的模型没有使用填充目标进行训练，并进行了长上下文的微调。

3. 代码Llama - Instruct 模型
    - 目标：这些模型是为了更好地理解和执行人类指令而设计的。
    - 模型规模：提供了7B、13B和34B参数的模型。
    - 训练方式：基于相应的代码Llama模型，并在额外的大约50亿标记的数据集上进行微调，以提高其对人类指令的理解能力。

4. 代码Llama 70B 特殊说明
    - 训练时间：70B模型是在7B、13B和34B模型之后几个月开始训练的。
    - 数据集大小：这个模型在1万亿标记的数据集上训练，比其他模型的数据集大一倍。
    - 特殊训练技术：
        - 使用了FIM（一种填充训练技术），这是34B模型中经常被要求的功能。
        - 只有基础的70B模型使用了LCFT（一种特殊的训练技术）进行训练。
        - 代码Llama - Instruct 70B：这个模型是从代码Llama - Python 70B模型训练而来的，这表明它在多种编程语言（包括Python）上的表现优于单纯的代码Llama 70B模型。

## 数据集
Code Llama 模型的训练：
- 模型大小：Code Llama 有 7B、13B、34B 和 70B 四个不同大小的版本。这个数字表示模型中的参数数量，B 代表 "billion"（十亿）。参数越多，模型的潜力越大，但计算和训练的成本也更高。

- 训练数据：
    - 7B、13B 和 34B 的模型训练在 5000 亿个 token 上，70B 的模型在 1 万亿个 token 上训练。这里的 token 可以理解为模型处理的基本单位，它可以是一个单词、子词或字符。
    - 去重数据集：为了避免重复的训练样本影响模型的学习，训练数据经过了近似去重处理。***去重是为了保证模型能学到多样化的代码和自然语言片段，避免过拟合。***
    - 代码数据：模型主要在公开的代码数据集上进行训练，这些数据集包含了丰富的代码样本。
    - 自然语言数据：为了提升模型在自然语言理解上的表现，8% 的数据来自与代码相关的自然语言数据集。这个数据集不仅包含了代码，还包含了围绕代码的讨论、解释和问题。

- Tokenization（标记化）：
    - 数据通过字节对编码（Byte Pair Encoding, BPE）进行标记化。BPE 是一种常见的标记化方法，它通过将常见的字符或字符组合逐步合并为一个 token，以有效地表示文本。
    - Tokenizer：使用与 Llama 和 Llama 2 相同的标记器，这确保了 Code Llama 和这些模型在处理文本时的一致性。

- MBPP 性能：MBPP 是 "Mostly Basic Programming Problems" 的缩写，是一个用于评估代码生成模型能力的基准数据集。初步实验表明，通过添加自然语言数据集的样本，Code Llama 在这个基准上的表现有所提升。

## 填充（Infilling）
- 代码填充任务：
    - 代码填充是一项预测程序中缺失部分的任务。它在实际应用中非常重要，比如在集成开发环境（IDEs）中的代码自动补全、类型推断以及生成代码文档等场景。

- 填充模型的训练：
    - 因果掩码：填充模型的训练基于因果掩码的概念。在这种方法中，训练序列的部分内容被移动到末尾，然后让模型自回归地预测整个重新排序的序列。
        1. 在 Code Llama 的填充任务中，因果掩码被用于训练模型在给定代码片段的上下文情况下填充缺失的代码。
        2. 具体做法是将代码序列拆分为前缀、中间部分和后缀，然后通过掩码确保模型只能基于前缀和后缀的信息去预测和填充中间部分的代码。
    - 分割方法：在训练过程中，文档在字符级别被拆分成三部分：前缀（prefix）、中间部分（middle part）和后缀（suffix）。这种分割是从文档长度的均匀分布中随机抽样的。

- 格式化：这些分割会被随机格式化为两种形式：
    - PSM 格式：前缀-后缀-中间（Prefix-Suffix-Middle）格式。
    - SPM 格式：后缀-前缀-中间（Suffix-Prefix-Middle）格式。这些格式的选择有助于模型更灵活地处理不同位置的代码片段。

- ***特殊标记：***
    - ***为了帮助模型识别不同部分的开始和结束位置，研究人员为 Llama 2 的标记器添加了四个特殊的标记。这些标记用来标记前缀、中间部分、后缀的开始以及填充范围的结束。***

- 避免分布偏差：
    - 在训练时，编码时通常会在标记前加一个隐含的空格（由 SentencePiece 标记器添加）。为了避免这种空格在填充任务中引入偏差，研究人员对中间部分和后缀进行了特殊处理，抑制了这些空格。

- 训练的细节：
    - 在 SPM 格式中，前缀和中间部分会在编码之前连接在一起，这样可以避免子标记的分裂。相比之下，在 PSM 格式中，模型仍然可能遇到被分裂的子标记。

## 长上下文微调（Long Context Fine-Tuning）
- 处理长序列的挑战：
    - Transformer 模型在处理长序列时面临两个主要挑战：
        1. 外推能力：模型能否在训练时从未见过的更长序列上正常工作。
        2. ***注意力机制的复杂性：模型的注意力机制在处理长序列时计算成本很高，这限制了模型通常只能在短至中等长度的输入上进行训练。***

- 长上下文微调：
    - 微调
        - 为了让 Code Llama 能够有效处理长序列，研究人员引入了一个专门的微调阶段。
        - 在这个阶段中，模型被训练来处理长达 16384 个 token 的序列，而 Llama 2 和初始训练阶段的序列长度限制为 4096 个 token。
    - 训练成本：通过仅在微调阶段处理长序列，研究人员在不显著增加训练成本的情况下增强了模型的长距离处理能力。

- 旋转位置嵌入的修改：
    - Llama 2 使用了旋转位置嵌入（Rotary Position Embedding, RPE）来增强模型的位置信息。为了适应长序列，研究人员对这种嵌入进行了修改。
    - 频率调整：与之前提出的方法不同，研究人员通过修改生成旋转频率的基准周期来增强模型的长序列处理能力。具体来说，基准周期从 10000 增加到 1000000。这个变化使得模型能够处理更长的序列，并减少了对短距离注意力的偏向。

- 实验结果：
    实验表明，Code Llama 在微调后的长序列处理能力有所提升，并且能够稳定地处理长达 100000 个 token 的超长序列。这展示了模型的强大外推能力。

## 指令微调

指令微调（Instruction Fine-tuning）是对模型进行特定任务调整的重要步骤，通过使用包含指令和回答示例的数据集，使模型能够更好地理解和响应用户的指令。在 Code Llama 的指令微调中，模型不仅需要生成代码，还需要适当地回答关于代码的问题。以下是详细解释：

1. 指令微调的目标
指令微调的主要目标是增强模型对用户指令的理解和响应能力，特别是在编码任务和与代码相关的自然语言任务中。通过指令微调，Code Llama - Instruct 模型可以更好地处理多种类型的任务，如代码生成、问题回答、代码解释等。

2. 训练数据集，指令微调过程中使用了三种不同类型的数据：
    1. 代码数据集：包括了大量的代码样本，这些样本占据了训练数据的很大一部分（85%），用于确保模型能够生成和理解代码。
    2. 与代码相关的自然语言数据：这些数据包括代码讨论、问题和答案等，这部分数据占训练数据的 8%，用于帮助模型理解和处理自然语言中与代码相关的内容。
    3. 一般自然语言数据集：这部分数据用于保持模型的自然语言理解能力，即使在专注于代码的任务上，模型也不会失去对一般自然语言的理解能力。这部分数据占 7%。

    ![dataset](/imgs/dataset.png)

    在指令微调的过程中，这些数据集以不同的比例进行采样和训练，以确保模型在多个任务上的表现平衡。

3. 专有数据集的使用
    - 为了进一步提升模型的指令遵循性，Code Llama - Instruct 使用了专有的数据集，这些数据集来自 Llama 2 的指令调优数据集，特别是“RLHF V5”版本。这个数据集通过多阶段的强化学习和人类反馈收集而成，包括：

    1. 监督微调数据：由人类专家生成的示例，用于引导模型的训练方向。
    2. 拒绝采样数据：在多个生成结果中，通过奖励模型选择最优的输出，这些输出在帮助性和安全性方面经过了严格筛选。
    3. 这些数据的使用使得 Code Llama - Instruct 能够继承 Llama 2 的指令遵循性和安全特性，能够更好地理解用户的意图并生成合适的响应。

4. 自我指令（Self-Instruct）
    - 由于编码任务的数据较少，收集这类数据代价高昂，因此在指令微调中引入了自我指令机制。具体步骤如下：

    1. 生成编程问题：通过提示 Llama 2 70B，生成了 62,000 个面试风格的编程问题。这些问题模拟了真实编程面试中可能遇到的问题。
    2. 去重问题：通过去除完全重复的问题，保留了约 52,000 个独特问题。
    3. 生成单元测试和解决方案：对于每个问题，通过提示 Code Llama 7B 生成单元测试和多个（10个）Python 解决方案。然后运行单元测试，并将第一个通过测试的解决方案（及其对应的问题和测试）添加到自我指令数据集中。

    这种自我指令的方法有效利用了模型自身生成的数据，减少了对人类专家的依赖，同时也提高了模型的训练效率。

5. 复习训练（Rehearsal）
    - 为了防止模型在训练过程中遗忘原有的编码和自然语言理解能力，Code Llama - Instruct 在微调时也混合了一些代码数据（6%）和自然语言数据（2%）。
    - 这部分数据用于“复习”模型之前学到的知识，确保模型在新任务上表现良好的同时，不会在旧任务上出现退步。

6. 效果与应用
经过这些步骤的指令微调，Code Llama - Instruct 模型在回答与编码相关的问题时表现更加出色，并且保持了对一般自然语言任务的强大能力。这使得该模型在实际应用中，能够处理广泛的任务，从代码生成到问题回答，再到代码解释和调试。
# Code Llama 概述

## Code Llama 介绍
- 大型语言模型已经在自然语言处理领域取得了显著的进步，这些模型可以执行多种任务，包括但不限于文本生成、对话系统、问答系统等。
- 它们通过在大量文本数据上进行训练来学习语言结构和模式。随着技术的进步，这些模型现在也被用于代码相关的任务，例如编写代码、代码补全、代码解释等。

## Code Llama 的背景
- Code Llama 是一组基于 Llama 2 的大型语言模型，专注于代码生成和代码补全任务。
- Llama 2 本身是一个通用的大型语言模型，它在广泛的文本数据上进行了预训练，包括一般的自然语言文本和代码数据。

## 训练方法
- AlphaCode、InCoder 和 StarCoder 等模型都是专门为代码生成而设计的，它们通常只在代码数据集上进行训练。
- Codex 则是从一个通用的语言模型出发，然后在这个基础上进行微调，使其适应代码相关的任务。
- Code Llama 同样采取类似的方法，即从一个已经预训练过的通用模型      开始，然后通过在代码数据上进行额外的训练和微调，使模型能够更好地理解和生成代码。

## 训练策略
- 初始化：Code Llama 使用 Llama 2 作为起点，这意味着它在开始专注于代码之前就已经具备了一定的自然语言处理能力。
- 训练流程：模型经历了多阶段的训练过程，包括基础训练、代码训练以及可能的进一步微调，以提高其在特定代码任务上的性能。
- 性能对比：通过实验比较发现，使用 Llama 2 进行初始化的 Code Llama 模型，在同样的训练资源条件下，相较于直接从零开始仅在代码数据上训练的模型，表现更好。

## 训练细节
- 这些模型在含有 16,000 个令牌的序列上进行了训练，并且在处理长达 100,000 个令牌的输入时显示出更好的性能。
- 特别是，70 亿、130 亿和 700 亿参数的 Code Llama 和 Code Llama - Instruct 变体支持基于周围内容的代码填充功能。

## 性能指标
- HumanEval：评估模型能否根据自然语言描述正确生成函数实现。
- MBPP (Microsoft's Benchmark for Programming Problems)：衡量模型解决编程问题的能力。
- MultiPL-E：一个多语言编程基准测试。

## 填充（Infilling）
- 定义：填充是指在已知文本序列中填入缺失的部分，同时考虑到上下文的一致性和逻辑性。
- 问题：传统的自回归训练（即基于先前的词汇预测下一个词汇的方法）和微调过程使模型能够完成给定的提示，但通常缺乏在考虑整体上下文的情况下填补缺失文本片段的能力。
- 解决方案：通过多任务学习方法，结合<kbd>自回归预测</kbd>（即基于已有词汇预测后续词汇）和 <kbd>因果填充预测</kbd>（即在已知 **周围词汇** 的情况下预测缺失词汇），来增强模型的填充能力。
- 应用场景：这种技术特别适用于源代码编辑器中的实时代码补全功能或文档字符串的生成。

## 长输入上下文
- 挑战：为了处理更大范围的代码（例如整个项目或仓库），模型需要理解比单一函数或文件更长的上下文。
- 限制：原始的Llama 2模型支持的最大上下文长度为4,096个令牌（token）。
- 改进：通过调整旋转位置嵌入（RoPE）参数，增加了一个额外的微调阶段，从而扩展了模型的最大上下文长度到100,000个令牌。
- 效果：这使得模型能够有效地处理更长的上下文，对代码生成的质量影响较小。

## 指令微调
- 定义：指令微调是一种训练技术，通过这种方式，模型被指导执行特定任务，以改进其响应的准确性和适用性。
- 目的：提高模型的安全性、减少有害输出以及避免偏见。
- 方法：使用专有的指令数据集进行额外的微调，这些数据集包含特定的指令和预期的响应。
- 数据集：***使用了一种新的自我指令数据集，它是由向Llama 2提问编码问题，然后让Code Llama生成相关的单元测试和解决方案的方式构建的。***
- 效果：改进后的模型（Code Llama - Instruct）在真实世界测试、有害性检测和偏见基准上表现出更好的性能，同时对代码生成的质量影响较小。

## 模型变体
- 基础模型 (Code Llama)：通用的基础模型，适用于广泛的编程任务。
- Python 专项模型 (Code Llama - Python)：专门针对 Python 编程语言进行了优化，以提高在 Python 相关任务中的表现。
- 遵循指令的模型 (Code Llama - Instruct)：经过训练能够更好地理解并遵循用户给出的指令或提示。
- 每种类型的模型都有四个不同大小的版本，参数量分别为 70 亿、130 亿、340 亿和 700 亿个。
- 示意图：
![模型](/imgs/models.png)

## 应用场景
- 程序合成：根据自然语言描述生成相应的程序代码。
- 代码补全：自动完成部分已写好的代码。
- 调试辅助：帮助开发者找出并修复代码中的错误。
- 文档生成：自动生成代码注释或文档。


# Code Llama：专门为代码设计

## 代码Llama模型家族

1. 代码Llama 模型
    模型规模：该家族包含了四个不同规模的模型，分别是7B、13B、34B和70B参数。这些数字代表了模型中可训练参数的数量。
    - 训练方式：
        - 7B、13B和70B的模型使用了一种叫做 ***填充（infilling）*** 的目标进行训练。这种训练方法有助于模型在代码片段中间补全缺失的部分，使其非常适合在IDE中完成代码片段。
        - 34B模型则没有使用填充目标进行训练。
    - 初始化与数据集：
        - 所有的代码Llama模型都从Llama 2模型的权重开始训练。
        - 除70B模型外的所有模型都在包含大量代码的500亿标记数据集上进行训练。
        - 70B模型则在更大的数据集上训练，总共1万亿标记。
        - 长上下文处理：所有模型都经过了特殊的微调过程来处理长上下文，即能够理解并生成更长的代码段。

2. 代码Llama - Python 模型
    - 专业领域：这些模型专门针对Python代码生成进行了优化。
    - 模型规模：同样提供了7B、13B、34B和70B参数的不同规模。
    - 训练方式：
        - 这些模型从Llama 2模型初始化，并在500亿标记的代码Llama数据集上进行训练。
        - 在此基础上，还使用了一个包含100亿标记的Python代码数据集进行了额外的专业化训练。
        - 7B、13B和34B参数的模型没有使用填充目标进行训练，并进行了长上下文的微调。

3. 代码Llama - Instruct 模型
    - 目标：这些模型是为了更好地理解和执行人类指令而设计的。
    - 模型规模：提供了7B、13B和34B参数的模型。
    - 训练方式：基于相应的代码Llama模型，并在额外的大约50亿标记的数据集上进行微调，以提高其对人类指令的理解能力。

4. 代码Llama 70B 特殊说明
    - 训练时间：70B模型是在7B、13B和34B模型之后几个月开始训练的。
    - 数据集大小：这个模型在1万亿标记的数据集上训练，比其他模型的数据集大一倍。
    - 特殊训练技术：
        - 使用了FIM（一种填充训练技术），这是34B模型中经常被要求的功能。
        - 只有基础的70B模型使用了LCFT（一种特殊的训练技术）进行训练。
        - 代码Llama - Instruct 70B：这个模型是从代码Llama - Python 70B模型训练而来的，这表明它在多种编程语言（包括Python）上的表现优于单纯的代码Llama 70B模型。